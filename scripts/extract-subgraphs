#!/usr/bin/env python3

# the make_graph() function is adapted from the mrs-to-penman project:
#   Project: https://github.com/goodmami/mrs-to-penman
#   License: https://github.com/goodmami/mrs-to-penman/blob/master/LICENSE

USAGE = '''
Usage: extract-subgraphs [--parameters=PATH] [--alignments=PATH]
                         [--annotate=MODE]
                         [--source-result-id=N] [--target-result-id=N]
                         PROFILE1 [PROFILE2]

Arguments:
  PROFILE1              an XMT or [incr tsdb()] profile
  PROFILE2              if given, a parallel profile to PROFILE1;
                        extract matching bilingual subgraphs

Options:
  -h, --help            display this help and exit
  --parameters PATH     JSON file of conversion parameters
  --annotate MODE       predicate annotations; values of MODE are:
                        hb: Haugereid & Bond (verb@1x, nmz_verb, nmd_"Kim")
                        xmt: XMT style (pron(3.sg.m), card("1"), etc.)
                        short: short form of predicates only
                        none: full pred strings only [default: none]
  --source-result-id N  use the Nth source result [default: 0]
  --target-result-id N  use the Nth target result [default: 0]
  --alignments PATH     use bilingual alignments at PATH;
                        assumes PROFILE2

'''

import sys
import os
import re
import json
from collections import Counter
from itertools import product, combinations

import docopt

from delphin.mrs import xmrs, simplemrs, penman, query
from delphin.mrs.components import var_sort, Pred
from delphin import itsdb

from util import rows, aligned_rows, predlist, read_anymalign_model

EncodeError = penman.penman.EncodeError
Triple = penman.penman.Triple
Graph = penman.penman.Graph


_DEFAULT_MAXDEPTH = 3
_DEFAULT_MIN_SIZE_RATIO = 0.1
_DEFAULT_MAX_SIZE_RATIO = 10


def xmrs_relation_sort(triples):
    return sorted(
        triples,
        key=lambda t: (
            t.inverted,
            t.relation.isupper(),
            t.relation == 'RSTR-H',
            [int(t) if t.isdigit() else t 
             for t in re.split(r'([0-9]+)', t.relation or '')]
        )
    )


codec = penman.XMRSCodec(indent=False, relation_sort=xmrs_relation_sort)


def main():
    args = docopt.docopt(USAGE)

    params = {}
    if args['--parameters']:
        params = json.load(open(args['--parameters']))

    tpid = args['--target-result-id']
    spid = args['--source-result-id']

    ann_mode = args['--annotate']

    source = itsdb.ItsdbProfile(args['PROFILE1'])

    if args['PROFILE2']:
        target = itsdb.ItsdbProfile(args['PROFILE2'])
        if args['--alignments']:
            alns = load_alignments(
                args['--alignments'],
                params.get('alignments', {}),
                ann_mode
            )
            data = extract_aligned_subgraphs(
                source, spid, target, tpid, alns, ann_mode, params
            )
        else:
            data = extract_bilingual_subgraphs(
                source, spid, target, tpid, params
            )
    else:
        data = extract_monolingual_subgraphs(
            source, spid, params
        )

    graph_filter = make_graph_filters(params.get('alignments', {}))

    commonfeats = [('count', 1)]
    if args['PROFILE2']:
        commonfeats.extend([('src-res-id', spid), ('tgt-res-id', tpid)])
    else:
        commonfeats.append(('res-id', spid))

    print('filtering extracted subgraphs', file=sys.stderr)
    for i_id, gs in data:
        for g1, g2, d in filter(graph_filter, gs):
            try:
                lin_g1 = codec.encode(g1)
                lin_g2 = None
                if g2 is not None:  # None for single-profile extraction
                    lin_g2 = codec.encode(g2)
                meta = [('id', i_id)] + commonfeats
                meta.extend(sorted(d.items()))
                print('# ' + ' '.join('::{} {}'.format(k, v) for k, v in meta))
                print(lin_g1)
                if lin_g2:
                    print(lin_g2)
                print()
            except EncodeError as ex:
                pass  # TODO: log warning


def extract_aligned_subgraphs(
        source,
        spid,
        target,
        tpid,
        alignments,
        ann_mode,
        params):
    print('extracting aligned subgraphs', file=sys.stderr)
    graphparams = params.get('graphs', {})
    dropset = set(graphparams.get('drop_nodes', []))
    for i_id, mrs1, mrs2 in aligned_rows(source, spid, target, tpid):
        print(' ', i_id, file=sys.stderr)

        x1 = simplemrs.loads_one(mrs1)
        x2 = simplemrs.loads_one(mrs2)
        g1 = make_graph(x1, graphparams)
        g2 = make_graph(x2, graphparams)
        x1_counts = Counter(predlist(x1, dropset, mode='short'))
        x2_counts = Counter(predlist(x2, dropset, mode='short'))

        g1_idkey = dict(
            (x[0], i) for i, x in enumerate(_traverse(g1, g1.top, -1))
        )
        g2_idkey = dict(
            (x[0], i) for i, x in enumerate(_traverse(g2, g2.top, -1))
        )
        if len(g1.variables()) > len(g1_idkey):
            continue  # g1 is probably disconnected
        if len(g2.variables()) > len(g2_idkey):
            continue  # g2 is probably disconnected

        gs = []
        for src_sig, tgts in alignments.items():
            # make sure source mrs is compatible
            if any(x1_counts[pred] < count for pred, count in src_sig):
                continue
            src_sgs = list(build_subgraphs_from_pred_sig(g1, src_sig, g1_idkey, graphparams))
            if not src_sgs:
                continue

            for tgt_sig, weights in tgts.items():
                # make sure target mrs is compatible
                if any(x2_counts[pred] < count for pred, count in tgt_sig):
                    continue
                tgt_sgs = list(build_subgraphs_from_pred_sig(g2, tgt_sig, g2_idkey, graphparams))
                if not tgt_sgs:
                    continue

                lexwts, transprobs, freq = weights
                if lexwts is None:
                    lexwts = (None, None)
                data = {
                    'src-lexwt': lexwts[0],
                    'tgt-lexwt': lexwts[1],
                    'src-tprob': transprobs[0],
                    'tgt-tprob': transprobs[1],
                    'freq': freq
                }

                gs.extend([(a, b, data) for a, b in product(src_sgs, tgt_sgs)])

        yield (i_id, gs)


def extract_bilingual_subgraphs(source, spid, target, tpid, params):
    print('extracting bilingual subgraphs', file=sys.stderr)
    graphparams = params.get('graphs', {})
    maxdepth = graphparams.get('maximum-depth', _DEFAULT_MAXDEPTH)
    for i_id, mrs1, mrs2 in aligned_rows(source, spid, target, tpid):
        x1 = simplemrs.loads_one(mrs1)
        x2 = simplemrs.loads_one(mrs2)
        g1 = make_graph(x1, graphparams)
        g2 = make_graph(x2, graphparams)
        sgs1 = list(enumerate_subgraphs(g1, maxdepth))
        sgs2 = list(enumerate_subgraphs(g2, maxdepth))
        gs = [(sg1, sg2, {}) for sg1 in sgs1 for sg2 in sgs2]
        yield (i_id, gs)


def extract_monolingual_subgraphs(source, pid, params):
    print('extracting monolingual subgraphs', file=sys.stderr)
    graphparams = params.get('graphs', {})
    maxdepth = graphparams.get('maximum-depth', _DEFAULT_MAXDEPTH)
    for i_id, p_id, mrs in rows(source):
        if p_id != pid:
            continue
        x = simplemrs.loads_one(mrs)
        g = make_graph(x, graphparams)
        gs = [(g, None, {}) for g in enumerate_subgraphs(g, maxdepth)]
        yield (i_id, gs)


def load_alignments(path, params, ann_mode):
    print('loading alignments', file=sys.stderr)
    model = {}
    min_freq = params.get('minimum-frequency', 0)
    min_lexwt = params.get('minimum-lexical-weight')
    min_tprob = params.get('minimum-translation-probability')
    mdl = read_anymalign_model(path)
    for src, tgtdata in mdl.items():
        src = _clean_annotations(src, ann_mode)
        src = tuple(sorted(Counter(src).items()))
        for tgt, lexwts, transprobs, freq in tgtdata:
            # simple filtering of the model
            if freq < min_freq:
                continue
            if (min_lexwt is not None and lexwts is not None and
                    (lexwts[0] < min_lexwt or lexwts[1] < min_lexwt)):
                continue
            if (min_tprob is not None and
                    (transprobs[0] < min_tprob or transprobs[1] < min_tprob)):
                continue
            tgt = _clean_annotations(tgt, ann_mode)
            # add to the model
            tgt = tuple(sorted(Counter(tgt).items()))
            if src not in model:
                model[src] = {}
            model[src][tgt] = (lexwts, transprobs, freq)

    return model


def _clean_annotations(preds, ann_mode):
    clean = []
    for p in preds:
        if ann_mode == 'hb':
            # haugereid and bond annotations
            # ep arity: not sure what to do with this, so discard
            p = re.sub(r'@(\d+[ehipxu])*$', '', p)
            # named rel with CARG
            p = re.sub(r'^nmd_"(.*)"$', r'named', p)
            if p.startswith('nmz_'):
                clean.append('nominalization')
                p = p[4:]
            p = Pred.string_or_grammar_pred(p).short_form()
        elif ann_mode == 'xmt':
            p = re.sub(r'(.*)\([^)]*\)$', r'\1', p)
        elif ann_mode != 'short':
            p = Pred.string_or_grammar_pred(p).short_form()
        clean.append(p)
    return tuple(clean)

# def pred_sig_to_predlist(sig):
#     predlist = []
#     for pred, count in sig:
#         predlist.extend([pred] * count)
#     return predlist


def build_subgraphs_from_pred_sig(g, sig, idkey, params):
    ids = dict((pred, []) for pred, _ in sig)
    for t in g.attributes(relation='predicate'):
        if t.target in ids:
            ids[t.target].append(t.source)

    for _ids in _node_combinations(ids, list(sig)):
        _ids = sorted(_ids, key=idkey.__getitem__)
        makevar = _make_makevar(g, _ids)
        ts = frontier(g, _ids)
        yield relabel_ids(Graph(ts, top=_ids[0]), makevar)


def _node_combinations(d, ps):
    if not ps:
        return [[]]
    pred, count = ps.pop()
    ids = [
        _ids + list(__ids)
        for _ids in _node_combinations(d, ps)
        for __ids in combinations(d[pred], count)
    ]
    return ids


def make_graph(x, params):
    # first create a graph to get normalized triples
    g = codec.triples_to_graph(xmrs.Dmrs.to_triples(x, properties=True))
    # then filter if necessary
    if params:
        varsort = dict((ep.nodeid, var_sort(ep.intrinsic_variable))
                       for ep in x.eps())
        preds = dict((nid, x.pred(nid).short_form()) for nid in x.nodeids())
        drop = set(params.get('drop_nodes', []))
        allow = params.get('allow_relations', {})
        global_allow = set(allow.get('global', []))
        x_allow = set(allow.get('x', []))
        e_allow = set(allow.get('e', []))
        pred_allow = allow.get('predicate', {})
        ts = [
            t for t in g.triples()
            if (preds.get(t.source) not in drop and
                preds.get(t.target) not in drop and
                (t.relation in global_allow or
                 (varsort.get(t.source) == 'x' and t.relation in x_allow) or
                 (varsort.get(t.source) == 'e' and t.relation in e_allow) or
                 (t.relation in pred_allow.get(preds.get(t.source), []))))
        ]
        top = g.top if g.top in [t.source for t in ts] else None
        g = codec.triples_to_graph(ts, top=top)

    return g


def enumerate_subgraphs(g, maxdepth):
    seen = set()
    for root in g.variables():
        ids = _traverse(g, root, maxdepth)

        makevar = _make_makevar(g, [i for i, _ in ids])
    
        sgdepth = max(d for _, d in ids)
        for depth in range(sgdepth + 1):
            _ids = [i for i, d in ids if d <= depth]
            triples = frontier(g, _ids)
            yield relabel_ids(Graph(triples, top=_ids[0]), makevar)


def _traverse(g, top, maxdepth, remaining=None, depth=0):
    if remaining is None:
        remaining = set(g.variables())
    vs = []
    if top in remaining and depth != maxdepth:
        vs.append((top, depth))
        remaining.remove(top)
        for t in g.edges(source=top):
            # debugging
            # if t.relation == 'R':
            #     raise Exception('{}, {}, {}'.format(t.source ,t.relation, t.target))
            if t.relation == 'RSTR-H' or t.relation.endswith('-EQ'):
                continue
            vs.extend(_traverse(g, t.target, maxdepth, remaining, depth+1))
        for t in g.edges(target=top):
            if t.relation.endswith('-EQ'):
                vs.extend(_traverse(g, t.source, maxdepth, remaining, depth+1))
        # do this separate from the -EQ ones so it's at the end of the list;
        # this is to help ensure the variable ID is the highest number
        # because I serialize the quantifiers on a node last
        for t in g.edges(target=top, relation='RSTR-H'):
            vs.extend(_traverse(g, t.source, maxdepth, remaining, depth+1))
    return vs


def _make_makevar(g, ids):
    idmap = dict(zip(ids, range(len(ids))))
    ivmap = dict((t.source, t.target)
                 for t in g.attributes(relation='cvarsort'))
    return lambda i, ivm=ivmap, idm=idmap: ivm.get(i, 'u') + str(idm[i])


def frontier(g, ids):
    # print(codec.encode(g))
    # print(ids)
    idset = set(ids)
    triples = []
    for t in g.attributes():
        if t.source in idset and t.relation != 'cvarsort':
            triples.append(
                Triple(t.source, t.relation, t.target, False)
            )
    for t in g.edges():
        if t.source in idset and t.target in idset:
            inverted = t.relation == 'RSTR-H' or t.relation.endswith('-EQ')
            triples.append(
                Triple(t.source, t.relation, t.target, inverted)

            )
    return triples


def relabel_ids(g, var):
    triples = []
    for t in g.attributes():
        if t.relation != 'cvarsort':
            triples.append(
                Triple(var(t.source), t.relation, t.target, False)
            )
    for t in g.edges():
        triples.append(
            Triple(var(t.source), t.relation, var(t.target), t.inverted)
        )
    return Graph(triples, top=var(g.top))


def make_graph_filters(params):
    same_top_var = params.get('same-top-variable-type', False)
    min_ratio = params.get('minimum-graph-size-ratio', _DEFAULT_MIN_SIZE_RATIO)
    max_ratio = params.get('maximum-graph-size-ratio', _DEFAULT_MAX_SIZE_RATIO)

    def graph_filter(gs):
        g1, g2, d = gs

        if same_top_var and var_sort(g1.top) != var_sort(g2.top):
            return False
        
        g1_len = len(g1.variables())
        g2_len = len(g2.variables())
        ratio = float(g1_len) / g2_len
        if not min_ratio <= ratio <= max_ratio:
            return False

        return True

    return graph_filter


if __name__ == '__main__':
    main()
